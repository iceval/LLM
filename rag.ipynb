{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install datasets==3.0.2 langchain==0.3.4 peft==0.13.2 python_docx==1.1.2 transformers==4.46.0 protobuf==3.20.3 sentencepiece==0.2.0 nltk==3.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from docx import Document\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "def parse_doc_to_sentences(filename = 'document.docx'):\n",
    "    \"\"\"\n",
    "    Прочитать документ (по умолчанию 'document.docx') и разбить его на предложения\n",
    "    \"\"\"\n",
    "    doc = Document(filename)\n",
    "    \n",
    "    text = \" \".join([paragraph.text for paragraph in doc.paragraphs if paragraph.text])\n",
    "    \n",
    "    return sent_tokenize(text)\n",
    "\n",
    "############################################################################################################\n",
    "# Определение моделей и функций, используемых для нахождения схожих предложений\n",
    "sentence_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sentence_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Использовать gpu по возможности\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_embedding(sentence):\n",
    "    \"\"\"\n",
    "    Получение эмбеддинга предложения\n",
    "    \"\"\"\n",
    "    #Mean Pooling - учет attention mask для корректного усреднения\n",
    "    def _mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] # Первый элемент model_output содержит эмбеддинги токенов\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Токенизация предложения\n",
    "    encoded_input = sentence_tokenizer([sentence], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Вычисление эмбеддингов токенов\n",
    "    with torch.no_grad():\n",
    "        model_output = sentence_model(**encoded_input)\n",
    "\n",
    "    # Выполнение пулинга\n",
    "    sentence_embeddings = _mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Нормализация эмбеддингов\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "def cosine_similarity(query_embedding, embeddings):\n",
    "    \"\"\"\n",
    "    Косинусное сходство\n",
    "    \"\"\"\n",
    "    # Нормализация эмбеддингов\n",
    "    query_norm = query_embedding / query_embedding.norm(dim=1, keepdim=True)\n",
    "    embeddings_norm = embeddings / embeddings.norm(dim=1, keepdim=True)\n",
    "    # Расчет косинусного сходства\n",
    "    similarity = torch.mm(query_norm, embeddings_norm.T)\n",
    "    return similarity\n",
    "\n",
    "def retrieve_similar_sentences(query, embeddings, sentences, k=5):\n",
    "    \"\"\"\n",
    "    Отбор более подходящих предложений\n",
    "    \"\"\"\n",
    "    # Получение эмбеддингов для запроса\n",
    "    query_embedding = get_embedding(query)\n",
    "    # Вычисление косинусного сходства\n",
    "    similarity = cosine_similarity(query_embedding, embeddings)\n",
    "    # Получение индексов наиболее похожих предложений\n",
    "    top_k_indices = similarity[0].topk(k).indices\n",
    "    # Возвращение соответствующих предложений\n",
    "    retrieved_sentences = [sentences[i] for i in top_k_indices]\n",
    "    return retrieved_sentences\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "# Определение моделей и функций, используемых для генерации текста на основе контекста\n",
    "adapt_model_name = \"IlyaGusev/saiga_mistral_7b_lora\"\n",
    "base_model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              base_model_name,\n",
    "              trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device_map = {\"\": 0} if torch.cuda.is_available() else None\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "              adapt_model_name,\n",
    "              device_map=device_map,\n",
    "              torch_dtype=torch.bfloat16)\n",
    "\n",
    "info_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    <s>user\n",
    "    Текст: {context}\n",
    "    Вопрос: {question}</s>\n",
    "    <s>bot\n",
    "    Ответ:</s>\"\"\")\n",
    "\n",
    "def get_answer(context, question):\n",
    "    \"\"\"\n",
    "    Получение ответа на основе контекста\n",
    "    \"\"\"\n",
    "    prompt = info_prompt.format(context=context, question=question)   \n",
    "    print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), \n",
    "                            top_p=0.5,\n",
    "                            temperature=0.3,\n",
    "                            attention_mask=inputs[\"attention_mask\"],\n",
    "                            max_new_tokens=50,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            do_sample=True)\n",
    "\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    parsed_answer = output.split(\"Ответ:\")[1].strip()\n",
    "\n",
    "    if \"bot\\n\" in parsed_answer:\n",
    "        parsed_answer = parsed_answer.replace(\"bot\\n\", \"\").strip()\n",
    "\n",
    "    return parsed_answer\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# Основная программа\n",
    "if __name__ == \"__main__\":\n",
    "    # Парсинг документа в массив предложений и создание эмбеддингов\n",
    "    filename = './document.docx'\n",
    "    sentences = parse_doc_to_sentences(filename)\n",
    "    embedding = torch.cat([get_embedding(sentence) for sentence in sentences])\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Задайте вопрос: \") #Например, \"РТУ МИРЭА это?\"\n",
    "        retrieved_sentences = retrieve_similar_sentences(query, embedding, sentences)\n",
    "        answer = get_answer(retrieved_sentences, query)\n",
    "        print(\"Ответ: \" + answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
